<!DOCTYPE HTML>

<div  style="background-color:#E9FFFF;">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soumya Shamarao Jahagirdar</title>
  
  <meta name="author" content="Soumya Shamarao Jahagirdar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <div>
                <p style="text-align:center">
                  <name style="font-family:verdana;"><b>Soumya Shamarao Jahagirdar</b></name>
                </p>
                <p style="font-family:verdana;">I am a Master's by Research student at CVIT, IIIT Hyderabad, India. I work under the guidance of Prof. C V Jawahar.
                </p>
                <p style="font-family:verdana;">
                  
                Previously I have worked with Prof. Shankar Gangisetty on text-based multimodal learning. I have also worked as a Research Assistant with Prof. B A Patil at Think & Ink Education and Research Foundation, on magnitude spectrum and descriptive statistics on color models for soil classification.

                I was a part of mentors at Winter Workshop and an active member at Computer Vision and Graphics Lab, KLE Technological University, Hubballi, India.

                I completed my B.E. in Computer Science and Engineering from KLE Technological University in 2021.

                <p style="font-family:verdana;"> More about me!
                I make friends very easily. I love sketching, it takes me to a transe-phase. Animals have my heart and I look forward to groom them throughout my life. I occasionally sing (I have learnt Hidustani Music for over 10 years) and yeah, I can cook some great dishes. Walking, badminton, running, home workout sessions help me balance the amount of food I consume on daily basis. To know more you know where to ping me!
                </p>
                </p>
                <p style="text-align:center; font-family:verdana;">
                  <a href="soumyasj22@gmail.com" style="font-family:verdana;"> <b>Email</b></a> &nbsp/&nbsp
                  <a href="data/Soumya_Jahagirdar_Resume_3_11_2021.pdf" style="font-family:verdana;"><b>CV</b></a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=oA25i6QAAAAJ&hl=en" style="font-family:verdana;"><b>Google Scholar</b></a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/soumya-jahagirdar/" style="font-family:verdana;"><b>LinkedIn</b></a>&nbsp/&nbsp
                  <a href="https://github.com/soumyasj/" style="font-family:verdana;"><b>Github</b></a> &nbsp/&nbsp
                  <a href="https://twitter.com/soumyasj2222" style="font-family:verdana;"><b>Twitter</b></a>&nbsp/&nbsp
                  <a href="sketches/drawing.html" style="font-family:verdana;"><b>More Me!</b></a>

                </p>
            </div>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/soumya.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/soumya.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <p>
        <heading style="color:red; font-family:verdana;"> <b>News!</b></heading>
        <ul style="font-family:verdana;"> 
          <li> <span style="color: blue;"><b>Dec 2021</b></span>: Presented our work "Look, Read and Ask: Learning to Ask Questions by Reading Text in Images" at Vision India, ICVGIP, 2021.</li>
          <li> <span style="color: blue;"><b>Sep 2021</b></span>: Presented our work "Look, Read and Ask: Learning to Ask Questions by Reading Text in Images" at ICDAR, 2021.</li>
          <li> <span style="color: blue;"><b>August 2021</b></span>: Started Master's by Research at CVIT, IIIT Hyderabad.</li>
          <li> <span style="color: blue;"><b>June 2021</b></span>: My work "Look, Read and Ask: Learning to Ask Questions by Reading Text in Images" with Dr. Shankar Gangisetty (KLE Technological University, Hubballi, India)and Dr. Anand Mishra (IIT Jodhpur, India) which got accepted in ICDAR 2021. In this work the task was to generate question leveraging textual information present in the images. (ORAL Presentation)</li>
          <li> <span style="color: blue;"><b>June 2021</b></span>: Presented our work "DeepDNet" at WiCV workshop, CVPR, 2021.</li>
          <li> <span style="color: blue;"><b>April 2021</b></span>: A joint project from KLE Technological University and Samsung Research India Bangalore (SRIB) our work "DeepDNet: Deep Dense Network for Depth Completion Task" got accepted at Women in Computer Vision (WiCV) CVPR-Workshops, 2021.</li>
          <li> <span style="color: blue;"><b>November 2020</b></span>: RQ Patent filed on "Method for Depth completion Task" with Samsung Research India Bangalore (SRIB).</li>
        </ul>
        </p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading style="font-family:verdana;"><b>Research</b></heading>
              <p style="font-family:verdana;">
                My research interests lie in Computer Vision, Deep Learning, Machine Learning, Multimodal Learning and Natural Language Processing. This field of study excites me and pushes me to work much harder everyday.
              </p>
            </td>
          </tr>
        </tbody></table>

        <heading style="font-family:verdana;"><b>Publications</b></heading>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

     
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-86549-8_22" style="font-family:verdana;">
                <papertitle style="font-family:verdana;">Look, Read and Ask: Learning to Ask Questions by Reading Text in Images</papertitle>
              </a>
              <br>

              <p style="font-family:verdana;"> <strong>Soumya Shamarao Jahagirdar</strong>, Shankar Gangisetty, Anand Mishra. </p>
              <a href="https://sites.google.com/site/shankarsetty/research/textvqg" style="font-family:verdana;">project page</a> /
              <a href="https://www.youtube.com/watch?v=31C2wSzCxpM" style="font-family:verdana;">video</a> /
              <a href="https://github.com/soumyasj/textVQG" style="font-family:verdana;">code</a>

              <p style="font-family:verdana;">
              <b> Abstract: </b> We present a novel problem of text-based visual question generation or TextVQG in short. Given the recent growing interest of the document image analysis community in combining text understanding with conversational artificial intelligence, e.g., text-based visual question answering, TextVQG becomes an important task. TextVQG aims to generate a natural language question for a given input image and an automatically extracted text also known as OCR token from it such that the OCR token is an answer to the generated question. TextVQG is an essential ability for a conversational agent. However, it is challenging as it requires an in-depth understanding of the scene and the ability to semantically bridge the visual content with the text present in the image. To address TextVQG, we present an OCR-consistent visual question generation model that Looks into the visual content, Reads the scene text, and Asks a relevant and meaningful natural language question. We refer to our proposed model as OLRA. We perform an extensive evaluation of OLRA on two public benchmarks and compare them against baselines. Our model – OLRA automatically generates questions similar to the public text-based visual question answering datasets that were curated manually. Moreover, we ‘significantly’ outperform baseline approaches on the performance measures popularly used in text generation literature.
              </p>
        

              <a href="https://openaccess.thecvf.com/content/CVPR2021W/WiCV/papers/Hegde_DeepDNet_Deep_Dense_Network_for_Depth_Completion_Task_CVPRW_2021_paper.pdf" style="font-family:verdana;">
                <papertitle style="font-family:verdana;">DeepDNet: Deep Dense Network for Depth Completion Task</papertitle>
              </a>
              <br>

              <p style="font-family:verdana;">
              Girish Hegde, Tushar Pharale, <strong> Soumya Jahagirdar</strong>, Vaishakh Nargund, Ramesh Ashok Tabib, Uma Mudenagudi, Basavaraja Vandrotti, Ankit Dhiman 
              WiCV, CVPR, 2021. </p> 

              <p style="font-family:verdana;">
              <b> Abstract: </b> In this paper, we propose a Deep Dense Network for Depth Completion Task (DeepDNet) towards generating dense depth map using sparse depth and captured view. Wide variety of scene understanding applications such as 3Dreconstruction, mixed reality, robotics demand accurate and dense depth maps. Existing depth sensors capture accurate and reliable sparse depth and find challenges in acquiring dense depth maps. Towards this we plan to utilise the accurate sparse depth as input with RGB image to generate dense depth. We model the transformation of random sparse input to grid-based sparse input using Quad-tree decomposition. We propose Dense-Residual-Skip (DRS) Autoencoder along with an attention towards edge preservation using Gradient Aware Mean Squared Error (GAMSE) Loss. We demonstrate our results on the NYUv2 dataset and compare it with other state of the art methods. We also show our results on sparse depth captured by ARCore depth API with its dense depth map. Extensive experiments suggest consistent improvements over existing methods.
              </p>
   
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <p style="font-family:verdana;">
            The website has been designed based on <a href="https://jonbarron.info/">https://jonbarron.info/</a>. I thank him for the wonderful source code he provided. 
          </p>
        </tbody></table>
</body>

</html>

</div>
