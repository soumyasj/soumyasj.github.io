<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E00L9PT3V9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-E00L9PT3V9');
  </script>
  <title>Soumya Shamarao Jahagirdar</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Soumya Shamarao Jahagirdar" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://soumyasj.github.io/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Soumya Shamarao Jahagirdar
              </h1>
              <p>I am a PhD student at University of Tübingen, advised by Prof. <a href="https://hildekuehne.github.io/">Hilde Kuhene</a>. I am working on multimodal learning. I completed my Master's by Research from IIIT Hyderabad, India under the guidance of Prof. <a href="https://faculty.iiit.ac.in/~jawahar/">C V Jawahar </a> and Prof. <a href="http://www.cvc.uab.es/people/dimos/">Dimosthenis Karatzas</a>  from <a href="http://www.cvc.uab.es"> Computer Vision Center </a> (CVC), UAB, Spain. My thesis was cemtered around understanding and combining information in videos through visual and textual modalities for question-answering. 
              </p>
              <p>
                Previously in my undergraduate research, I have worked with Prof. <a href="https://sites.google.com/site/shankarsetty/home">Shankar Gangisetty</a> from KLE Technological University and Prof. <a href="https://anandmishra22.github.io/">Anand Mishra</a> on text-based multimodal learning, specifically, utilizing Scene-text in images for Text-based Visual Question Generation. I have also worked with Prof. <a href="https://www.cse.iitd.ac.in/~uma/">Uma Mudenagudi</a> and <a href="https://research.samsung.com/sri-b">Samsung R&D Institute India-Bangalore</a> on Depth Estimation and Densification. In my undergrad I also worked as a Research Assistant with Prof. <a href="https://www.linkedin.com/in/basawaraj-patil-6a4118118/?originalSubdomain=in">B A Patil</a> at Think & Ink Education and Research Foundation.
              </p>
         
              <p style="text-align:center">
                <a target="_blank" href="soumyasj22@gmail.com"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/soumyasj">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=oA25i6QAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/soumya-jahagirdar"> LinkedIn </a>&nbsp;/&nbsp;
                <a href="data/soumya_jahagirdar_resume_16_10_24.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/soumyasj2222">Twitter</a>&nbsp;&nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/updated_soumya.jpg">
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                My research interests lie in Computer Vision, Deep Learning, Machine Learning, Multimodal Learning and Natural Language Processing. This field of study excites me and pushes me to work much harder everyday.
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>News</h2>
              <p>
                <b><span style="color: #FF5733;">September 2024:</span></b> Started my PhD in University of Tübingen!
              </p>
              <p>
                <b><span style="color: #FF5733;">August 2024:</span></b> Our competition <b>ICDAR 2024 Competition on Reading Documents Through Aria Glasses</b> was presented by Dr. <a href="https://scholar.google.com/citations?user=ai9lifYAAAAJ&hl=en">Ajoy Mondal</a> at ICDAR in Athens, Greece!
              </p>
              <p>
                <b><span style="color: #FF5733;">June 2024:</span></b> Our paper <b>"Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal Videos"</b> got accepted at International Conference on Computer Vision & Image Processing - <b>CVIP</b>!
              </p>
              <p>
                <b><span style="color: #FF5733;">June 2024:</span></b> Our paper <b>"Prompt2LVideos: Exploring Prompts for Understanding Long-Form Multimodal Videos"</b> got accepted at International Conference on Computer Vision & Image Processing - <b>CVIP</b>!
              </p>
              <p>
                <b><span style="color: #FF5733;">April 2024:</span></b> Organizing a competition on <b>Reading documents through ARIA glasses</b> with <b>Meta Reality Labs</b>!
              </p>
              <p>
                <b><span style="color: #FF5733;">March 2024:</span></b> Successfully defended MS thesis titled: <b>Text-based Video Question Answering</b>!
              </p>
              <p>
                <b><span style="color: #FF5733;">February 2024:</span></b> Participated in Google Research Week in Bangalore!
              </p>
              <p>
                <b><span style="color: #FF5733;">September 2023:</span></b> Attended <b>International Space Conference</b> in collaboration with ISRO, InSpace, and NSIL!
              </p>
              <p>
                <b><span style="color: #FF5733;">August 2023:</span></b> Our paper <b>"Understanding Video Scenes through Text: Insights from Text-based Video Question Answering"</b> got accepted at ICCV Workshop (VLAR) 2023. <b><span style="color: #000000;">(Spotlight)</span></b>
              </p>
              <p>
                <b><span style="color: #FF5733;">March 2023:</span></b> Organized competition on <b>Video Question Answering on News videos</b> in <b>ICDAR</b> 2023.
              </p>
              <p>
                <b><span style="color: #FF5733;">February 2023:</span></b> Two papers got accepted in CVPR-2023 O-DRUM Workshop.
              </p>
              <p>
                <b><span style="color: #FF5733;">December 2022:</span></b> Student volunteer member at <b>ICFHR</b> conference 2023.
              </p>
              <p>
                <b><span style="color: #FF5733;">October 2022:</span></b> Our paper <b>"Watching the News: Towards VideoQA Models that can Read"</b> got accepted at <b>WACV</b>, 2023.
              </p>
              <p>
                <b><span style="color: #FF5733;">September 2022:</span></b> I started my internship at Computer Vision Center (CVC), UAB, Barcelona, Spain.
              </p>
              <p>
                <b><span style="color: #FF5733;">July 2022:</span></b> Conducted a tutorial on transformers in Summer School of AI, CVIT.
              </p>
              <p>
                <b><span style="color: #FF5733;">May 2022:</span></b> First Patent on <b>Single Image Depth Estimation with SRIB-Bangalore</b> got accpeted.
              </p>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/iccvw.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/iccvw.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</h3>
              <br>
              <strong>Soumya Shamarao Jahagirdar</strong>,  Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar 

              <br>
              <em>International Conference on Computer Vision <strong> (ICCV) </strong> Workshops, VLAR</em>, 2023
              <br>
              <p></p>
              <p>More details coming soon!</p>

            </td>
          </tr>
          
          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/charani.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/charani.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Weakly Supervised Visual Question Answer Generation</h3>
              <br>
              Charani Alampalle, Shamanthak Hegde, <strong>Soumya Shamarao Jahagirdar</strong>, Shankar Gangisetty

              <br>
              <em>Conference on Computer Vision and Pattern Recognition <strong> (CVPR) </strong> Workshops, ODRUM</em>, 2023
              <br>
              
              <a href="https://arxiv.org/abs/2306.06622">paper</a> 
              <p></p>
              <p>We propose a weakly-supervised visual question answer generation method that generates a relevant question-answer pairs for a given input image and associated caption.</p>

            </td>
          </tr>
   
          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/shamanthak.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/shamanthak.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Making the V in Text-VQA Matter</h3>
              <br>
              Shamanthak Hegde, <strong>Soumya Shamarao Jahagirdar</strong>, Shankar Gangisetty

              <br>
              <em>Conference on Computer Vision and Pattern Recognition <strong> (CVPR) </strong> Workshops, ODRUM</em>, 2023
              <br>
              
              <a href="https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Hegde_Making_the_V_in_Text-VQA_Matter_CVPRW_2023_paper.html">paper</a> 
              <p></p>
              <p>We propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA.</p>

            </td>
          </tr>

          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/newsvideoqa.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/newsvideoqa.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Watching the News: Towards VideoQA Models that can Read</h3>
              <br>
              <strong>Soumya Shamarao Jahagirdar</strong>,  Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar 

              <br>
              <em>Winter Conference on Applications of Computer Vision, <strong> WACV </strong></em>, 2023
              <br>
              
              <a href="https://arxiv.org/abs/2211.05588">paper</a> 
              / <a href="https://github.com/soumyasj/NewsVideoQA">code</a> 
              / <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/videoqa">website</a> 
              / <a href="https://www.youtube.com/watch?app=desktop&v=rnCCONldMik">youtube</a> 

              <p></p>
              <p>We propose a novel VideoQA task that requires reading and understanding the text in the video. We focus on news videos and require QA systems to comprehend and answer questions about the topics presented by combining visual and textual cues in the video. We introduce the “NewsVideoQA” dataset that comprises more than 8,600 QA pairs on 3,000+ news videos obtained from diverse news channels from around the world.</p>

            </td>
          </tr>
          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/textvqg.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/textvqg.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Look, Read and Ask: Learning to Ask Questions by Reading Text in Images</h3>
              <br>
              <strong>Soumya Shamarao Jahagirdar</strong>,  Shankar Gangisetty, Anand Mishra

              <br>
              <em>International Conference on Document Analysis and Recognition, <strong> (ICDAR) </strong></em>, 2021
              <br>
              
              <a href="https://arxiv.org/pdf/2211.12950.pdf">paper</a> 
              / <a href="https://github.com/soumyasj/textVQG">code</a> 
              / <a href="https://sites.google.com/site/shankarsetty/research/textvqg">website</a> 
              / <a href="https://www.youtube.com/watch?v=31C2wSzCxpM">youtube</a> 

              <p></p>
              <p>We present a novel problem of text-based visual question generation or TextVQG in short. Given the recent growing interest of the document image analysis community in combining text understanding with conversational artificial intelligence, e.g., text-based visual question answering, TextVQG becomes an important task. TextVQG aims to generate a natural language question for a given input image and an automatically extracted text also known as OCR token from it such that the OCR token is an answer to the generated question.</p>

            </td>
          </tr>

          <tr>
            <!-- <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/deepdnet.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td> -->
            <td style="padding: 2.5%; width: 40%; vertical-align: middle; min-width: 120px">
              <img src="/images/deepdnet.png" alt="project image" style="max-width: 100%; max-height: 100%; width: auto; height: auto;" />
            </td>
          
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>DeepDNet: Deep Dense Network for Depth Completion Task</h3>
              <br>
              Girish Hegde, Tushar "<strong>Soumya Shamarao Jahagirdar</strong>, Vaishakh Nargund, Ramesh Ashok Tabib, Uma Mudenagudi, Basavaraja Vandrotti, Ankit Dhiman"

              <br>
              <em>Conference on Computer Vision and Pattern Recognition <strong> (CVPR) </strong> Workshops, WiCV</em>, 2021
              <br>
              
              <a href="https://openaccess.thecvf.com/content/CVPR2021W/WiCV/html/Hegde_DeepDNet_Deep_Dense_Network_for_Depth_Completion_Task_CVPRW_2021_paper.html">paper</a> 
              <p></p>
              <p>We propose a Deep Dense Network for Depth Completion Task (DeepDNet) towards generating dense depth map using sparse depth and captured view. We propose Dense-Residual-Skip (DRS) Autoencoder along with an attention towards edge preservation using Gradient Aware Mean Squared Error (GAMSE) Loss.</p>

            </td>
          </tr>
        </table>
        <br>
        <br>
        <br> 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Patents</h2>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/patent.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Method and Device of Depth Densification using RGB Image and Sparse Depth</h3>
              <br>
              <em>patent </em>
              <br>
              2022-05-05

              <br>
              
              <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=oA25i6QAAAAJ&citation_for_view=oA25i6QAAAAJ:2osOgNQ5qMEC">website</a> 
              <p></p>
              <p>PATENT NUMBER: WO2022103171A1; PATENT OFFICE: US; PUBLICATION DATE: 2022/05/19; Inventors
Suhas MUDENAGUDI Uma, HEGDE Girish, Dattatray, Tabib Ramesh Ashok, JAHAGIRDAR Soumya, Shamarao, PHARALE Tushar, Irappa, Vandrotti Basavaraja, Shanthappa, Dhiman Ankit, NARGUND Vaishakh</p>

            </td>
          </tr> 
        </table>
        <br>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

